= Rerankers
:navtitle: Rerankers

Rerankers improve search result quality by reordering documents based on their relevance to a query using specialized models. RedisVL for Java provides powerful reranking capabilities through HuggingFace cross-encoder models.

== What are Rerankers?

Rerankers provide a relevance boost to search results generated by traditional (lexical) or semantic search strategies. While initial search retrieves a broad set of potentially relevant documents, rerankers apply more sophisticated relevance scoring to produce a refined, higher-quality ranking.

=== Cross-Encoders vs Bi-Encoders

RedisVL uses **cross-encoder** models for reranking:

* **Bi-encoders** (used for embeddings): Encode query and document separately, then compare vectors
* **Cross-encoders** (used for reranking): Encode query+document pairs together, producing direct relevance scores

Cross-encoders are slower but more accurate because they can model the interaction between query and document directly.

== Why Use Rerankers?

Reranking addresses common search quality issues:

* *Improve Precision* - Surface the most relevant results at the top
* *Better Ranking* - Cross-encoders understand query-document relationships better than vector similarity alone
* *Flexible Integration* - Apply reranking to any search results (vector, lexical, or hybrid)
* *Cost-Effective* - Rerank only the top K results rather than scoring all documents

Typical workflow:

. Perform fast initial search (vector or hybrid) to get top 100 candidates
. Use reranker to precisely rank top 10 results
. Return highest-quality results to users

== HFCrossEncoderReranker

The `HFCrossEncoderReranker` class uses real HuggingFace cross-encoder models running via ONNX Runtime. Models are automatically downloaded and cached locally.

=== Setup

Add ONNX Runtime and HuggingFace tokenizer dependencies to your project:

[tabs]
====
Maven::
+
[source,xml]
----
<dependency>
    <groupId>com.microsoft.onnxruntime</groupId>
    <artifactId>onnxruntime</artifactId>
    <version>1.20.0</version>
</dependency>
<dependency>
    <groupId>com.google.code.gson</groupId>
    <artifactId>gson</artifactId>
    <version>2.11.0</version>
</dependency>
<dependency>
    <groupId>ai.djl.huggingface</groupId>
    <artifactId>tokenizers</artifactId>
    <version>0.30.0</version>
</dependency>
----

Gradle::
+
[source,gradle]
----
implementation 'com.microsoft.onnxruntime:onnxruntime:1.20.0'
implementation 'com.google.code.gson:gson:2.11.0'
implementation 'ai.djl.huggingface:tokenizers:0.30.0'
----
====

=== Basic Usage

[source,java]
----
import com.redis.vl.utils.rerank.HFCrossEncoderReranker;
import com.redis.vl.utils.rerank.RerankResult;
import java.util.Arrays;
import java.util.List;

// Create reranker with default model
HFCrossEncoderReranker reranker = new HFCrossEncoderReranker();

// Define query and documents
String query = "What is the capital of the United States?";
List<String> docs = Arrays.asList(
    "Carson City is the capital city of Nevada.",
    "Washington, D.C. is the capital of the United States.",
    "Charlotte Amalie is the capital of the US Virgin Islands.",
    "Capital punishment exists in the United States."
);

// Rerank documents
RerankResult result = reranker.rank(query, docs);

// Access reranked documents
List<?> rerankedDocs = result.getDocuments();
System.out.println("Top result: " + rerankedDocs.get(0));

// Access relevance scores
if (result.hasScores()) {
    List<Double> scores = result.getScores();
    for (int i = 0; i < rerankedDocs.size(); i++) {
        System.out.println("Score: " + scores.get(i) + " - " + rerankedDocs.get(i));
    }
}
----

=== Understanding Relevance Scores

Relevance scores are probability values in the range [0, 1]:

* **1.0**: Perfect relevance (exact match)
* **0.5**: Moderate relevance
* **0.0**: No relevance

The scores are calculated by applying sigmoid activation to the model's raw outputs, matching the behavior of the Python `sentence-transformers` library. Higher scores indicate stronger query-document relevance.

NOTE: Scores are relative to the input documents. A score of 0.9 doesn't guarantee relevance - it means this document is highly relevant compared to others in the batch.

=== Builder Pattern

Configure the reranker with the builder:

[source,java]
----
HFCrossEncoderReranker reranker = HFCrossEncoderReranker.builder()
    .model("cross-encoder/ms-marco-MiniLM-L-6-v2")  // Model name
    .limit(5)                                        // Return top 5 results
    .returnScore(true)                               // Include relevance scores
    .cacheDir("/path/to/model/cache")               // Custom cache directory
    .build();
----

=== Supported Models

HFCrossEncoderReranker works with any HuggingFace cross-encoder that has ONNX exports. The implementation automatically detects the model architecture (BERT, XLMRoberta, RoBERTa) and handles tokenization accordingly.

Popular models include:

[cols="2,3,1"]
|===
| Model | Use Case | Size

| `cross-encoder/ms-marco-MiniLM-L-6-v2`
| General-purpose reranking (default)
| ~80MB

| `cross-encoder/ms-marco-MiniLM-L-12-v2`
| Higher accuracy general reranking
| ~130MB

| `cross-encoder/stsb-distilroberta-base`
| Semantic similarity scoring
| ~250MB

| `BAAI/bge-reranker-base`
| Multilingual reranking (XLMRoberta)
| ~280MB

| `BAAI/bge-reranker-large`
| Highest accuracy (slower)
| ~560MB
|===

Models are automatically downloaded from HuggingFace and cached in `~/.cache/redisvl4j/` by default.

NOTE: Both BERT-based models (e.g., ms-marco-MiniLM) and XLMRoberta-based models (e.g., BAAI/bge-reranker) are fully supported with automatic architecture detection.

=== Working with String Documents

The simplest form accepts a list of strings:

[source,java]
----
List<String> docs = Arrays.asList(
    "Redis is an in-memory database",
    "PostgreSQL is a relational database",
    "MongoDB is a document database"
);

RerankResult result = reranker.rank("What is Redis?", docs);

// Returns List<String> when input was List<String>
List<String> rerankedDocs = (List<String>) result.getDocuments();
----

=== Working with Map Documents

For structured documents, use maps with a `content` field:

[source,java]
----
import java.util.Map;

List<Map<String, Object>> docs = Arrays.asList(
    Map.of("id", "doc1", "content", "Redis is an in-memory database", "source", "wiki"),
    Map.of("id", "doc2", "content", "PostgreSQL is a relational database", "source", "docs"),
    Map.of("id", "doc3", "content", "MongoDB is a document database", "source", "wiki")
);

RerankResult result = reranker.rank("What is Redis?", docs);

// Returns List<Map<String, Object>> with all fields preserved
List<Map<String, Object>> rerankedDocs =
    (List<Map<String, Object>>) result.getDocuments();

// Access full document with metadata
Map<String, Object> topDoc = rerankedDocs.get(0);
System.out.println("ID: " + topDoc.get("id"));
System.out.println("Content: " + topDoc.get("content"));
System.out.println("Source: " + topDoc.get("source"));
----

NOTE: Only documents with a `content` field are ranked. Documents missing this field are skipped.

=== Configuration Options

[source,java]
----
HFCrossEncoderReranker reranker = HFCrossEncoderReranker.builder()
    // Model selection
    .model("cross-encoder/ms-marco-MiniLM-L-6-v2")

    // Limit: Maximum number of results to return
    // Useful for reducing response size and computation
    .limit(10)

    // Return scores: Include relevance scores in results
    // Scores help you filter by confidence threshold
    .returnScore(true)

    // Cache directory: Where to store downloaded models
    // Default: ~/.cache/redisvl4j/
    .cacheDir(System.getProperty("user.home") + "/.cache/redisvl4j")

    .build();
----

=== Model Caching

Models are automatically cached after first download:

. First run: Downloads model from HuggingFace (~80MB for default model)
. Subsequent runs: Loads from local cache (fast)
. Cache location: `~/.cache/redisvl4j/models/<model-name>/`

[source,java]
----
// First time: Downloads model (one-time ~5-10 seconds)
HFCrossEncoderReranker reranker = new HFCrossEncoderReranker();

// Subsequent times: Loads from cache (instant)
HFCrossEncoderReranker reranker2 = new HFCrossEncoderReranker();
----

To use a custom cache directory:

[source,java]
----
String customCache = "/data/ml-models/cache";
HFCrossEncoderReranker reranker = HFCrossEncoderReranker.builder()
    .cacheDir(customCache)
    .build();
----

=== Resource Management

Rerankers hold ONNX Runtime sessions that should be cleaned up:

[source,java]
----
HFCrossEncoderReranker reranker = new HFCrossEncoderReranker();
try {
    // Use reranker
    RerankResult result = reranker.rank(query, docs);
} finally {
    // Clean up resources
    reranker.close();
}
----

== Integration with SearchIndex

Rerankers work seamlessly with RedisVL search results:

[source,java]
----
import com.redis.vl.index.SearchIndex;
import com.redis.vl.query.VectorQuery;

// Perform initial vector search (get top 100 candidates)
VectorQuery query = VectorQuery.builder()
    .vector(queryEmbedding)
    .field("embedding")
    .numResults(100)  // Broad initial retrieval
    .build();

List<Map<String, Object>> searchResults = index.query(query);

// Rerank to get best 10 results
HFCrossEncoderReranker reranker = HFCrossEncoderReranker.builder()
    .limit(10)
    .build();

RerankResult reranked = reranker.rank("user query text", searchResults);

// Present top 10 highest-quality results to user
List<Map<String, Object>> topResults =
    (List<Map<String, Object>>) reranked.getDocuments();
----

== Performance Considerations

=== Speed vs Accuracy

Cross-encoders are more accurate but slower than vector similarity:

* **Vector similarity**: ~1ms for 1000 documents (compare embeddings)
* **Cross-encoder reranking**: ~10-100ms for 100 documents (model inference)

Best practice: Use vector search for broad retrieval, then rerank top candidates.

=== Model Selection Trade-offs

[cols="2,1,1,1"]
|===
| Model | Speed | Accuracy | Size

| `ms-marco-MiniLM-L-6-v2`
| Fast
| Good
| Small

| `ms-marco-MiniLM-L-12-v2`
| Medium
| Better
| Medium

| `bge-reranker-large`
| Slow
| Best
| Large
|===

=== Batch Size Recommendations

For optimal performance:

* **Interactive queries**: Rerank top 10-20 candidates
* **Batch processing**: Rerank top 50-100 candidates
* **Maximum practical**: ~200 documents per query

=== Memory Usage

* Model loaded once per JVM: ~200-600MB RAM depending on model
* Inference per query: ~10-50MB temporary memory
* Models are cached on disk, not in memory between runs

== Error Handling

[source,java]
----
try {
    RerankResult result = reranker.rank(query, docs);
} catch (IllegalArgumentException e) {
    // Invalid arguments (null query, empty docs, etc.)
    System.err.println("Invalid input: " + e.getMessage());
} catch (RuntimeException e) {
    // Model loading or inference failure
    System.err.println("Reranking failed: " + e.getMessage());
}
----

Common errors:

* *Model not found*: Check model name and network connectivity
* *Out of memory*: Use smaller model or increase JVM heap
* *Invalid documents*: Ensure documents have `content` field for Map inputs

== Next Steps

* xref:getting-started.adoc[Getting Started Guide] - Basic vector search setup
* xref:hybrid-queries.adoc[Hybrid Queries] - Combine vector and metadata filtering
* xref:vectorizers.adoc[Vectorizers] - Create embeddings for initial search
* xref:api-reference.adoc[API Reference] - Complete Javadoc documentation
