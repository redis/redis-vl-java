{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Cache with RedisVL4J\n",
    "\n",
    "This notebook demonstrates how to use RedisVL4J's `SemanticCache` to efficiently cache LLM responses based on semantic similarity of queries.\n",
    "\n",
    "First, we will set up our dependencies and create a simple `askOpenAI` helper method to assist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load Maven dependencies\n",
    "%maven redis.clients:jedis:5.2.0\n",
    "%maven org.slf4j:slf4j-nop:2.0.16\n",
    "%maven com.fasterxml.jackson.core:jackson-databind:2.18.0\n",
    "%maven com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.18.0\n",
    "%maven com.github.f4b6a3:ulid-creator:5.2.3\n",
    "%maven dev.langchain4j:langchain4j:0.36.2\n",
    "%maven dev.langchain4j:langchain4j-open-ai:0.36.2\n",
    "%maven com.microsoft.onnxruntime:onnxruntime:1.16.3\n",
    "%maven com.squareup.okhttp3:okhttp:4.12.0\n",
    "%maven com.google.code.gson:gson:2.10.1\n",
    "\n",
    "// Note: RedisVL4J JAR must be in classpath (loaded automatically by Docker container)\n",
    "\n",
    "// Import RedisVL4J classes\n",
    "import com.redis.vl.extensions.cache.*;\n",
    "import com.redis.vl.utils.vectorize.*;\n",
    "\n",
    "// Import Redis client\n",
    "import redis.clients.jedis.UnifiedJedis;\n",
    "import redis.clients.jedis.HostAndPort;\n",
    "\n",
    "// Import LangChain4J\n",
    "import dev.langchain4j.model.openai.OpenAiLanguageModel;\n",
    "\n",
    "// Import Java standard libraries\n",
    "import java.util.*;\n",
    "import java.time.Duration;\n",
    "import java.util.function.Function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API configured\n"
     ]
    }
   ],
   "source": [
    "// Setup connection and initialize components\n",
    "UnifiedJedis jedis = new UnifiedJedis(new HostAndPort(\"redis-stack\", 6379));\n",
    "\n",
    "// Setup OpenAI client\n",
    "String apiKey = System.getenv(\"OPENAI_API_KEY\");\n",
    "OpenAiLanguageModel languageModel = OpenAiLanguageModel.builder()\n",
    "    .apiKey(apiKey)\n",
    "    .modelName(\"gpt-3.5-turbo-instruct\")\n",
    "    .timeout(Duration.ofSeconds(60))\n",
    "    .build();\n",
    "\n",
    "System.out.println(\"OpenAI API configured\");\n",
    "\n",
    "// Create the askOpenAI function\n",
    "Function<String, String> askOpenAI = (question) -> {\n",
    "    try {\n",
    "        return languageModel.generate(question).content().trim();\n",
    "    } catch (Exception e) {\n",
    "        throw new RuntimeException(\"Failed to call OpenAI: \" + e.getMessage(), e);\n",
    "    }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "// Test\n",
    "System.out.println(askOpenAI.apply(\"What is the capital of France?\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing `SemanticCache`\n",
    "\n",
    "`SemanticCache` will automatically create an index within Redis upon initialization for the semantic cache content.\n",
    "\n",
    "We'll use the `SentenceTransformersVectorizer` which downloads and runs HuggingFace models locally using ONNX Runtime. On first use, it will download the `Xenova/all-MiniLM-L6-v2` model (~25MB ONNX) and cache it locally in `~/.cache/redisvl4j/models/`. Subsequent runs will use the cached model for fast initialization.\n",
    "\n",
    "Note: We're using `Xenova/all-MiniLM-L6-v2` which is an ONNX-optimized version of the popular all-MiniLM-L6-v2 model. The original `redis/langcache-embed-v3` model uses SafeTensors format which requires additional conversion support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vectorizer with Xenova/all-MiniLM-L6-v2 model...\n",
      "Model dimensions: 384\n",
      "SemanticCache initialized with index: llmcache\n"
     ]
    }
   ],
   "source": [
    "// Create vectorizer using SentenceTransformersVectorizer to download and run model locally\n",
    "// Note: Using Xenova/all-MiniLM-L6-v2 which has ONNX support in the main directory\n",
    "// The redis/langcache-embed-v3 model uses SafeTensors format which is not yet supported\n",
    "BaseVectorizer vectorizer = new SentenceTransformersVectorizer(\"Xenova/all-MiniLM-L6-v2\");\n",
    "\n",
    "System.out.println(\"Initializing vectorizer with Xenova/all-MiniLM-L6-v2 model...\");\n",
    "System.out.println(\"Model dimensions: \" + vectorizer.getDimensions());\n",
    "\n",
    "// Initialize SemanticCache using Builder pattern\n",
    "SemanticCache llmcache = new SemanticCache.Builder()\n",
    "    .name(\"llmcache\")                    // underlying search index name\n",
    "    .redisClient(jedis)                  // redis connection\n",
    "    .distanceThreshold(0.1f)             // semantic cache distance threshold  \n",
    "    .vectorizer(vectorizer)              // embedding model\n",
    "    .build();\n",
    "\n",
    "System.out.println(\"SemanticCache initialized with index: \" + llmcache.getName());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache index 'llmcache' is ready for use\n"
     ]
    }
   ],
   "source": [
    "// Look at the index specification created for the semantic cache lookup\n",
    "System.out.println(\"Cache index '\" + llmcache.getName() + \"' is ready for use\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cache Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "String question = \"What is the capital of France?\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty cache\n"
     ]
    }
   ],
   "source": [
    "// Check the semantic cache -- should be empty\n",
    "Optional<CacheHit> response = llmcache.check(question);\n",
    "if (response.isPresent()) {\n",
    "    System.out.println(response.get());\n",
    "} else {\n",
    "    System.out.println(\"Empty cache\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial cache check should be empty since we have not yet stored anything in the cache. Below, store the `question`, proper `response`, and any arbitrary `metadata` (as a Java Map object) in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored in cache\n"
     ]
    }
   ],
   "source": [
    "// Cache the question, answer, and arbitrary metadata\n",
    "Map<String, Object> metadata = new HashMap<>();\n",
    "metadata.put(\"city\", \"Paris\");\n",
    "metadata.put(\"country\", \"france\");\n",
    "\n",
    "llmcache.store(question, \"Paris\", metadata);\n",
    "System.out.println(\"Stored in cache\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the cache again with the same question and with a semantically similar question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in cache:\n",
      "  Prompt: What is the capital of France?\n",
      "  Response: Paris\n",
      "  Distance: 0.0\n",
      "  Metadata: {country=france, vector_distance=0, updated_at=1758775412, city=Paris, id=2fecdce0-a4f7-4349-b61f-7b4b5ec8d6c2, inserted_at=1758775412}\n"
     ]
    }
   ],
   "source": [
    "// Check the cache again\n",
    "Optional<CacheHit> cacheResponse = llmcache.check(question);\n",
    "if (cacheResponse.isPresent()) {\n",
    "    CacheHit hit = cacheResponse.get();\n",
    "    System.out.println(\"Found in cache:\");\n",
    "    System.out.println(\"  Prompt: \" + hit.getPrompt());\n",
    "    System.out.println(\"  Response: \" + hit.getResponse());\n",
    "    System.out.println(\"  Distance: \" + hit.getDistance());\n",
    "    System.out.println(\"  Metadata: \" + hit.getMetadata());\n",
    "} else {\n",
    "    System.out.println(\"Empty cache\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "// Check for a semantically similar result\n",
    "String similarQuestion = \"What actually is the capital of France?\";\n",
    "Optional<CacheHit> similarResponse = llmcache.check(similarQuestion);\n",
    "if (similarResponse.isPresent()) {\n",
    "    System.out.println(similarResponse.get().getResponse());\n",
    "} else {\n",
    "    System.out.println(\"Not found in cache\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize the Distance Threshold\n",
    "\n",
    "For most use cases, the right semantic similarity threshold is not a fixed quantity. Depending on the choice of embedding model, the properties of the input query, and even business use case -- the threshold might need to change.\n",
    "\n",
    "Fortunately, you can seamlessly adjust the threshold at any point like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance threshold set to 0.5\n"
     ]
    }
   ],
   "source": [
    "// Widen the semantic distance threshold\n",
    "llmcache.setDistanceThreshold(0.5f);\n",
    "System.out.println(\"Distance threshold set to 0.5\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "// Really try to trick it by asking around the point\n",
    "// But is able to slip just under our new threshold\n",
    "String trickQuestion = \"What is the capital city of the country in Europe that also has a city named Nice?\";\n",
    "Optional<CacheHit> trickResponse = llmcache.check(trickQuestion);\n",
    "if (trickResponse.isPresent()) {\n",
    "    System.out.println(trickResponse.get().getResponse());\n",
    "} else {\n",
    "    System.out.println(\"Not found in cache\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache after clear: Empty\n"
     ]
    }
   ],
   "source": [
    "// Invalidate the cache completely by clearing it out\n",
    "llmcache.clear();\n",
    "\n",
    "// Should be empty now\n",
    "Optional<CacheHit> clearedResponse = llmcache.check(trickQuestion);\n",
    "System.out.println(\"Cache after clear: \" + (clearedResponse.isPresent() ? \"Not empty\" : \"Empty\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize TTL\n",
    "\n",
    "Redis uses TTL policies (optional) to expire individual keys at points in time in the future. This allows you to focus on your data flow and business logic without bothering with complex cleanup tasks.\n",
    "\n",
    "A TTL policy set on the `SemanticCache` allows you to temporarily hold onto cache entries. Below, we will set the TTL policy to 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created cache with 5 second TTL\n"
     ]
    }
   ],
   "source": [
    "// Create a new cache with TTL\n",
    "SemanticCache ttlCache = new SemanticCache.Builder()\n",
    "    .name(\"llmcache_ttl\")\n",
    "    .redisClient(jedis)\n",
    "    .distanceThreshold(0.1f)\n",
    "    .vectorizer(vectorizer)\n",
    "    .ttl(5) // 5 seconds\n",
    "    .build();\n",
    "\n",
    "System.out.println(\"Created cache with 5 second TTL\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored entry with TTL\n"
     ]
    }
   ],
   "source": [
    "ttlCache.store(\"This is a TTL test\", \"This is a TTL test response\");\n",
    "System.out.println(\"Stored entry with TTL\");\n",
    "\n",
    "Thread.sleep(6000); // Sleep for 6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after TTL expiry: Empty (expired)\n"
     ]
    }
   ],
   "source": [
    "// Confirm that the cache has cleared by now on its own\n",
    "Optional<CacheHit> ttlResult = ttlCache.check(\"This is a TTL test\");\n",
    "\n",
    "System.out.println(\"Result after TTL expiry: \" + (ttlResult.isPresent() ? \"Found\" : \"Empty (expired)\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Clean up TTL cache\n",
    "ttlCache.clear();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Performance Testing\n",
    "\n",
    "Next, we will measure the speedup obtained by using `SemanticCache`. We will use timing to measure the time taken to generate responses with and without `SemanticCache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Helper function to answer a simple question using OpenAI with a wrapper\n",
    " * check for the answer in the semantic cache first.\n",
    " */\n",
    "java.util.function.Function<String, String> answerQuestion = (q) -> {\n",
    "    Optional<CacheHit> results = llmcache.check(q);\n",
    "    if (results.isPresent()) {\n",
    "        return results.get().getResponse();\n",
    "    } else {\n",
    "        String answer = askOpenAI.apply(q);\n",
    "        return answer;\n",
    "    }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without caching, a call to OpenAI to answer this simple question took 0.684 seconds.\n",
      "Added to cache\n"
     ]
    }
   ],
   "source": [
    "long start = System.currentTimeMillis();\n",
    "// asking a question -- openai response time\n",
    "String perfQuestion = \"What was the name of the first US President?\";\n",
    "String answer = answerQuestion.apply(perfQuestion);\n",
    "long end = System.currentTimeMillis();\n",
    "\n",
    "double timeWithoutCache = (end - start) / 1000.0;\n",
    "System.out.println(\"Without caching, a call to OpenAI to answer this simple question took \" + timeWithoutCache + \" seconds.\");\n",
    "\n",
    "// add the entry to our LLM cache\n",
    "llmcache.store(perfQuestion, \"George Washington\");\n",
    "System.out.println(\"Added to cache\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time taken with LLM cache enabled: 0.0241\n",
      "Percentage of time saved: 96.48%\n"
     ]
    }
   ],
   "source": [
    "// Calculate the avg latency for caching over LLM usage\n",
    "List<Double> times = new ArrayList<>();\n",
    "\n",
    "for (int i = 0; i < 10; i++) {\n",
    "    long cachedStart = System.currentTimeMillis();\n",
    "    String cachedAnswer = answerQuestion.apply(perfQuestion);\n",
    "    long cachedEnd = System.currentTimeMillis();\n",
    "    times.add((cachedEnd - cachedStart) / 1000.0);\n",
    "}\n",
    "\n",
    "double avgTimeWithCache = times.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);\n",
    "double percentageSaved = ((timeWithoutCache - avgTimeWithCache) / timeWithoutCache) * 100;\n",
    "\n",
    "System.out.println(\"Avg time taken with LLM cache enabled: \" + avgTimeWithCache);\n",
    "System.out.println(\"Percentage of time saved: \" + String.format(\"%.2f%%\", percentageSaved));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Statistics:\n",
      "Hit count: 13\n",
      "Miss count: 3\n",
      "Hit rate: 81.25%\n"
     ]
    }
   ],
   "source": [
    "// Check the stats of the cache\n",
    "System.out.println(\"\\nCache Statistics:\");\n",
    "System.out.println(\"Hit count: \" + llmcache.getHitCount());\n",
    "System.out.println(\"Miss count: \" + llmcache.getMissCount());\n",
    "System.out.println(\"Hit rate: \" + String.format(\"%.2f%%\", llmcache.getHitRate() * 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared\n"
     ]
    }
   ],
   "source": [
    "// Clear the cache (but don't delete the index)\n",
    "llmcache.clear();\n",
    "System.out.println(\"Cache cleared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Access Controls, Tags & Filters\n",
    "\n",
    "When running complex workflows with similar applications, or handling multiple users it's important to keep data segregated. Building on top of RedisVL4J's support for complex and hybrid queries we can tag and filter cache entries.\n",
    "\n",
    "Let's store multiple users' data in our cache with similar prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored user-specific cache entries\n"
     ]
    }
   ],
   "source": [
    "// Store entries with user metadata\n",
    "Map<String, Object> userAbc = new HashMap<>();\n",
    "userAbc.put(\"user\", \"abc\");\n",
    "\n",
    "Map<String, Object> userDef = new HashMap<>();\n",
    "userDef.put(\"user\", \"def\");\n",
    "\n",
    "llmcache.store(\n",
    "    \"What is the phone number linked to my account?\",\n",
    "    \"The number on file is 123-555-0000\",\n",
    "    userAbc\n",
    ");\n",
    "\n",
    "llmcache.store(\n",
    "    \"What's the phone number linked in my account?\",\n",
    "    \"The number on file is 123-555-1111\",\n",
    "    userDef\n",
    ");\n",
    "\n",
    "System.out.println(\"Stored user-specific cache entries\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found entry: The number on file is 123-555-0000\n"
     ]
    }
   ],
   "source": [
    "// Check cache entries\n",
    "Optional<CacheHit> phoneResponse = llmcache.check(\n",
    "    \"What is the phone number linked to my account?\"\n",
    ");\n",
    "\n",
    "if (phoneResponse.isPresent()) {\n",
    "    System.out.println(\"Found entry: \" + phoneResponse.get().getResponse());\n",
    "} else {\n",
    "    System.out.println(\"No entry found\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All caches cleaned up and connection closed.\n",
      "SemanticCache demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "// Final cleanup - clear cache and close connection\n",
    "llmcache.clear();\n",
    "jedis.close();\n",
    "\n",
    "System.out.println(\"\\nAll caches cleaned up and connection closed.\");\n",
    "System.out.println(\"SemanticCache demonstration complete!\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "21.0.8+9-Ubuntu-0ubuntu124.04.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
