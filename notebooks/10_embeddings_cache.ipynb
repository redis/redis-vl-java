{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching Embeddings with RedisVL\n",
    "\n",
    "RedisVL provides an `EmbeddingsCache` that makes it easy to store and retrieve embedding vectors with their associated text and metadata. This cache is particularly useful for applications that frequently compute the same embeddings, enabling you to:\n",
    "\n",
    "- Reduce computational costs by reusing previously computed embeddings\n",
    "- Decrease latency in applications that rely on embeddings\n",
    "- Store additional metadata alongside embeddings for richer applications\n",
    "\n",
    "This notebook will show you how to use the `EmbeddingsCache` effectively in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries. We'll use a LangChain4J embedding model to generate our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load Maven dependencies\n",
    "%maven redis.clients:jedis:6.2.0\n",
    "%maven org.slf4j:slf4j-nop:2.0.16\n",
    "%maven com.fasterxml.jackson.core:jackson-databind:2.18.0\n",
    "%maven com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.18.0\n",
    "%maven com.github.f4b6a3:ulid-creator:5.2.3\n",
    "%maven dev.langchain4j:langchain4j:0.36.2\n",
    "%maven dev.langchain4j:langchain4j-embeddings-all-minilm-l6-v2:0.36.2\n",
    "\n",
    "// Import RedisVL classes\n",
    "import com.redis.vl.extensions.cache.*;\n",
    "import com.redis.vl.utils.vectorize.*;\n",
    "\n",
    "// Import Redis client\n",
    "import redis.clients.jedis.UnifiedJedis;\n",
    "import redis.clients.jedis.HostAndPort;\n",
    "\n",
    "// Import LangChain4J\n",
    "import dev.langchain4j.model.embedding.onnx.allminilml6v2.AllMiniLmL6V2EmbeddingModel;\n",
    "\n",
    "// Import Java standard libraries\n",
    "import java.util.*;\n",
    "import java.util.stream.Collectors;\n",
    "import java.time.Duration;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vectorizer to generate embeddings for our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer initialized: all-minilm-l6-v2\n",
      "Dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "// Initialize the vectorizer\n",
    "BaseVectorizer vectorizer = new LangChain4JVectorizer(\n",
    "    \"all-minilm-l6-v2\",\n",
    "    new AllMiniLmL6V2EmbeddingModel(),\n",
    "    384,\n",
    "    \"float32\"\n",
    ");\n",
    "\n",
    "System.out.println(\"Vectorizer initialized: \" + vectorizer.getModelName());\n",
    "System.out.println(\"Dimensions: \" + vectorizer.getDimensions());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the EmbeddingsCache\n",
    "\n",
    "Now let's initialize our `EmbeddingsCache`. The cache requires a Redis connection to store the embeddings and their associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingsCache initialized with prefix: embedcache\n"
     ]
    }
   ],
   "source": [
    "// Create Redis connection\n",
    "UnifiedJedis jedis = new UnifiedJedis(new HostAndPort(\"redis-stack\", 6379));\n",
    "\n",
    "// Initialize the embeddings cache\n",
    "EmbeddingsCache cache = new EmbeddingsCache(\n",
    "    \"embedcache\",     // name prefix for Redis keys\n",
    "    jedis,            // Redis connection\n",
    "    null              // Optional TTL in seconds (null means no expiration)\n",
    ");\n",
    "\n",
    "System.out.println(\"EmbeddingsCache initialized with prefix: embedcache\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### Storing Embeddings\n",
    "\n",
    "Let's store some text with its embedding in the cache. The `set` method takes the following parameters:\n",
    "- `text`: The input text that was embedded\n",
    "- `modelName`: The name of the embedding model used\n",
    "- `embedding`: The embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored embedding for: What is machine learning?\n",
      "Model name: all-minilm-l6-v2\n",
      "Embedding dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "// Text to embed\n",
    "String text = \"What is machine learning?\";\n",
    "String modelName = vectorizer.getModelName();\n",
    "\n",
    "// Generate the embedding\n",
    "float[] embedding = vectorizer.embed(text);\n",
    "\n",
    "// Store in cache\n",
    "cache.set(text, modelName, embedding);\n",
    "\n",
    "System.out.println(\"Stored embedding for: \" + text);\n",
    "System.out.println(\"Model name: \" + modelName);\n",
    "System.out.println(\"Embedding dimensions: \" + embedding.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Embeddings\n",
    "\n",
    "To retrieve an embedding from the cache, use the `get` method with the original text and model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in cache: What is machine learning?\n",
      "Model: all-minilm-l6-v2\n",
      "Embedding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "// Retrieve from cache\n",
    "Optional<float[]> result = cache.get(text, modelName);\n",
    "if (result.isPresent()) {\n",
    "    System.out.println(\"Found in cache: \" + text);\n",
    "    System.out.println(\"Model: \" + modelName);\n",
    "    \n",
    "    float[] cachedEmbedding = result.get();\n",
    "    System.out.println(\"Embedding shape: (\" + cachedEmbedding.length + \",)\");\n",
    "} else {\n",
    "    System.out.println(\"Not found in cache.\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Existence\n",
    "\n",
    "You can check if an embedding exists in the cache without retrieving it using the `exists` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First query exists in cache: true\n",
      "New query exists in cache: false\n"
     ]
    }
   ],
   "source": [
    "// Check if existing text is in cache\n",
    "boolean exists = cache.exists(text, modelName);\n",
    "System.out.println(\"First query exists in cache: \" + exists);\n",
    "\n",
    "// Check if a new text is in cache\n",
    "String newText = \"What is deep learning?\";\n",
    "exists = cache.exists(newText, modelName);\n",
    "System.out.println(\"New query exists in cache: \" + exists);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Entries\n",
    "\n",
    "To remove an entry from the cache, use the `drop` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping: false\n"
     ]
    }
   ],
   "source": [
    "// Remove from cache\n",
    "cache.drop(text, modelName);\n",
    "\n",
    "// Verify it's gone\n",
    "exists = cache.exists(text, modelName);\n",
    "System.out.println(\"After dropping: \" + exists);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "### Key-Based Operations\n",
    "\n",
    "The `EmbeddingsCache` uses SHA-256 hashing internally to generate keys from text and model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored embedding for: What is machine learning?\n",
      "Entry exists: true\n",
      "Entry dropped\n"
     ]
    }
   ],
   "source": [
    "// Store an entry again\n",
    "cache.set(text, modelName, embedding);\n",
    "System.out.println(\"Stored embedding for: \" + text);\n",
    "\n",
    "// Check existence\n",
    "boolean existsNow = cache.exists(text, modelName);\n",
    "System.out.println(\"Entry exists: \" + existsNow);\n",
    "\n",
    "// Drop the entry\n",
    "cache.drop(text, modelName);\n",
    "System.out.println(\"Entry dropped\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Operations\n",
    "\n",
    "When working with multiple embeddings, batch operations can significantly improve performance by reducing network roundtrips. The `EmbeddingsCache` provides methods prefixed with `m` (for \"multi\") that handle batches efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 embeddings with batch operation\n",
      "All embeddings exist: true\n",
      "Retrieved 3 embeddings in one operation\n",
      "Deleted all embeddings with batch operation\n"
     ]
    }
   ],
   "source": [
    "// Create multiple embeddings\n",
    "List<String> texts = List.of(\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is deep learning?\"\n",
    ");\n",
    "\n",
    "List<float[]> embeddings = texts.stream()\n",
    "    .map(t -> vectorizer.embed(t))\n",
    "    .collect(Collectors.toList());\n",
    "\n",
    "// Prepare batch items for mset\n",
    "Map<String, float[]> embeddingMap = new HashMap<>();\n",
    "for (int i = 0; i < texts.size(); i++) {\n",
    "    embeddingMap.put(texts.get(i), embeddings.get(i));\n",
    "}\n",
    "\n",
    "// Store multiple embeddings in one operation\n",
    "cache.mset(embeddingMap, modelName);\n",
    "System.out.println(\"Stored \" + embeddingMap.size() + \" embeddings with batch operation\");\n",
    "\n",
    "// Check if multiple embeddings exist in one operation\n",
    "Map<String, Boolean> existResults = cache.mexists(texts, modelName);\n",
    "boolean allExist = existResults.values().stream().allMatch(Boolean::booleanValue);\n",
    "System.out.println(\"All embeddings exist: \" + allExist);\n",
    "\n",
    "// Retrieve multiple embeddings in one operation\n",
    "Map<String, float[]> results = cache.mget(texts, modelName);\n",
    "System.out.println(\"Retrieved \" + results.size() + \" embeddings in one operation\");\n",
    "\n",
    "// Delete multiple embeddings in one operation\n",
    "cache.mdrop(texts, modelName);\n",
    "System.out.println(\"Deleted all embeddings with batch operation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch operations are particularly beneficial when working with large numbers of embeddings. They provide the same functionality as individual operations but with better performance by reducing network roundtrips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TTL (Time-To-Live)\n",
    "\n",
    "You can set a global TTL when initializing the cache, or specify TTL for individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immediately after setting: true\n",
      "After waiting: false\n"
     ]
    }
   ],
   "source": [
    "// Create a cache with a default 5-second TTL\n",
    "EmbeddingsCache ttlCache = new EmbeddingsCache(\n",
    "    \"ttl_cache\",\n",
    "    jedis,\n",
    "    5  // 5 second TTL\n",
    ");\n",
    "\n",
    "// Store an entry\n",
    "String ttlText = \"This is a TTL test\";\n",
    "float[] ttlEmbedding = vectorizer.embed(ttlText);\n",
    "ttlCache.set(ttlText, modelName, ttlEmbedding);\n",
    "\n",
    "// Check if it exists\n",
    "boolean exists = ttlCache.exists(ttlText, modelName);\n",
    "System.out.println(\"Immediately after setting: \" + exists);\n",
    "\n",
    "// Wait for it to expire\n",
    "Thread.sleep(6000); // Sleep for 6 seconds\n",
    "\n",
    "// Check again\n",
    "exists = ttlCache.exists(ttlText, modelName);\n",
    "System.out.println(\"After waiting: \" + exists);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also override the default TTL for individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry with custom TTL after 2 seconds: false\n",
      "Entry with default TTL after 2 seconds: true\n"
     ]
    }
   ],
   "source": [
    "// Store an entry with a custom 1-second TTL\n",
    "String shortLivedText = \"Short-lived entry\";\n",
    "float[] shortLivedEmbedding = vectorizer.embed(shortLivedText);\n",
    "ttlCache.setWithTTL(\n",
    "    shortLivedText,\n",
    "    modelName,\n",
    "    shortLivedEmbedding,\n",
    "    1  // Override with 1 second TTL\n",
    ");\n",
    "\n",
    "// Store another entry with the default TTL (5 seconds)\n",
    "String defaultTTLText = \"Default TTL entry\";\n",
    "float[] defaultTTLEmbedding = vectorizer.embed(defaultTTLText);\n",
    "ttlCache.set(\n",
    "    defaultTTLText,\n",
    "    modelName,\n",
    "    defaultTTLEmbedding\n",
    "    // No TTL specified = uses the default 5 seconds\n",
    ");\n",
    "\n",
    "// Wait for 2 seconds\n",
    "Thread.sleep(2000);\n",
    "\n",
    "// Check both entries\n",
    "boolean exists1 = ttlCache.exists(shortLivedText, modelName);\n",
    "boolean exists2 = ttlCache.exists(defaultTTLText, modelName);\n",
    "\n",
    "System.out.println(\"Entry with custom TTL after 2 seconds: \" + exists1);\n",
    "System.out.println(\"Entry with default TTL after 2 seconds: \" + exists2);\n",
    "\n",
    "// Cleanup\n",
    "if (exists2) {\n",
    "    ttlCache.drop(defaultTTLText, modelName);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example\n",
    "\n",
    "Let's build a simple embeddings caching system for a text classification task. We'll check the cache before computing new embeddings to save computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "Total queries: 5\n",
      "Cache hits: 2\n",
      "Cache misses: 3\n",
      "Cache hit rate: 40.0%\n",
      "Cache cleaned up\n"
     ]
    }
   ],
   "source": [
    "// Create a fresh cache for this example\n",
    "EmbeddingsCache exampleCache = new EmbeddingsCache(\n",
    "    \"example_cache\",\n",
    "    jedis,\n",
    "    3600  // 1 hour TTL\n",
    ");\n",
    "\n",
    "// Create a vectorizer with cache integration\n",
    "BaseVectorizer cachedVectorizer = new LangChain4JVectorizer(\n",
    "    \"all-minilm-l6-v2\",\n",
    "    new AllMiniLmL6V2EmbeddingModel(),\n",
    "    384,\n",
    "    \"float32\"\n",
    ");\n",
    "cachedVectorizer.setCache(exampleCache);\n",
    "\n",
    "// Simulate processing a stream of queries\n",
    "List<String> queries = List.of(\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is artificial intelligence?\",  // Repeated query\n",
    "    \"What are neural networks?\",\n",
    "    \"How does machine learning work?\"   // Repeated query\n",
    ");\n",
    "\n",
    "// Process the queries and track statistics\n",
    "int totalQueries = 0;\n",
    "int cacheHits = 0;\n",
    "\n",
    "for (String query : queries) {\n",
    "    totalQueries++;\n",
    "    \n",
    "    // Check cache before computing\n",
    "    boolean before = exampleCache.exists(query, cachedVectorizer.getModelName());\n",
    "    if (before) {\n",
    "        cacheHits++;\n",
    "    }\n",
    "    \n",
    "    // Get embedding (will compute or use cache)\n",
    "    float[] queryEmbedding = cachedVectorizer.embed(query);\n",
    "}\n",
    "\n",
    "// Report statistics\n",
    "int cacheMisses = totalQueries - cacheHits;\n",
    "double hitRate = (cacheHits / (double) totalQueries) * 100;\n",
    "\n",
    "System.out.println(\"\\nStatistics:\");\n",
    "System.out.println(\"Total queries: \" + totalQueries);\n",
    "System.out.println(\"Cache hits: \" + cacheHits);\n",
    "System.out.println(\"Cache misses: \" + cacheMisses);\n",
    "System.out.println(\"Cache hit rate: \" + String.format(\"%.1f\", hitRate) + \"%\");\n",
    "\n",
    "// Cleanup\n",
    "Set<String> uniqueQueries = new HashSet<>(queries);\n",
    "for (String query : uniqueQueries) {\n",
    "    exampleCache.drop(query, cachedVectorizer.getModelName());\n",
    "}\n",
    "\n",
    "System.out.println(\"Cache cleaned up\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "Let's run benchmarks to compare the performance of embedding with and without caching, as well as batch versus individual operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking without caching:\n",
      "Time taken without caching: 0.079 seconds\n",
      "Average time per embedding: 0.0079 seconds\n",
      "\n",
      "Benchmarking with caching:\n",
      "Time taken with caching: 0.028 seconds\n",
      "Average time per embedding: 0.0028 seconds\n",
      "\n",
      "Performance comparison:\n",
      "Speedup with caching: 2.82x faster\n",
      "Time saved: 0.051 seconds (64.6%)\n",
      "Latency reduction: 0.0051 seconds per query\n"
     ]
    }
   ],
   "source": [
    "// Text to use for benchmarking\n",
    "String benchmarkText = \"This is a benchmark text to measure the performance of embedding caching.\";\n",
    "\n",
    "// Create a fresh cache for benchmarking\n",
    "EmbeddingsCache benchmarkCache = new EmbeddingsCache(\n",
    "    \"benchmark_cache\",\n",
    "    jedis,\n",
    "    3600  // 1 hour TTL\n",
    ");\n",
    "\n",
    "// Create vectorizer with cache\n",
    "BaseVectorizer benchmarkVectorizer = new LangChain4JVectorizer(\n",
    "    \"all-minilm-l6-v2\",\n",
    "    new AllMiniLmL6V2EmbeddingModel(),\n",
    "    384,\n",
    "    \"float32\"\n",
    ");\n",
    "benchmarkVectorizer.setCache(benchmarkCache);\n",
    "\n",
    "// Number of iterations for the benchmark\n",
    "int nIterations = 10;\n",
    "\n",
    "// Benchmark without caching\n",
    "System.out.println(\"Benchmarking without caching:\");\n",
    "long startTime = System.currentTimeMillis();\n",
    "for (int i = 0; i < nIterations; i++) {\n",
    "    float[] uncachedEmbedding = benchmarkVectorizer.embed(benchmarkText, null, false, true); // skip cache\n",
    "}\n",
    "long noCacheTime = System.currentTimeMillis() - startTime;\n",
    "System.out.println(\"Time taken without caching: \" + (noCacheTime / 1000.0) + \" seconds\");\n",
    "System.out.println(\"Average time per embedding: \" + (noCacheTime / 1000.0 / nIterations) + \" seconds\");\n",
    "\n",
    "// Benchmark with caching\n",
    "System.out.println(\"\\nBenchmarking with caching:\");\n",
    "startTime = System.currentTimeMillis();\n",
    "for (int i = 0; i < nIterations; i++) {\n",
    "    float[] cachedEmbedding = benchmarkVectorizer.embed(benchmarkText);\n",
    "}\n",
    "long cacheTime = System.currentTimeMillis() - startTime;\n",
    "System.out.println(\"Time taken with caching: \" + (cacheTime / 1000.0) + \" seconds\");\n",
    "System.out.println(\"Average time per embedding: \" + (cacheTime / 1000.0 / nIterations) + \" seconds\");\n",
    "\n",
    "// Compare performance\n",
    "double speedup = (double) noCacheTime / cacheTime;\n",
    "double latencyReduction = (noCacheTime / 1000.0 / nIterations) - (cacheTime / 1000.0 / nIterations);\n",
    "System.out.println(\"\\nPerformance comparison:\");\n",
    "System.out.println(\"Speedup with caching: \" + String.format(\"%.2f\", speedup) + \"x faster\");\n",
    "System.out.println(\"Time saved: \" + ((noCacheTime - cacheTime) / 1000.0) + \" seconds (\" + \n",
    "                   String.format(\"%.1f\", (1 - (double) cacheTime / noCacheTime) * 100) + \"%)\");\n",
    "System.out.println(\"Latency reduction: \" + String.format(\"%.4f\", latencyReduction) + \" seconds per query\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Use Cases for Embedding Caching\n",
    "\n",
    "Embedding caching is particularly useful in the following scenarios:\n",
    "\n",
    "1. **Search applications**: Cache embeddings for frequently searched queries to reduce latency\n",
    "2. **Content recommendation systems**: Cache embeddings for content items to speed up similarity calculations\n",
    "3. **API services**: Reduce costs and improve response times when generating embeddings through paid APIs\n",
    "4. **Batch processing**: Speed up processing of datasets that contain duplicate texts\n",
    "5. **Chatbots and virtual assistants**: Cache embeddings for common user queries to provide faster responses\n",
    "6. **Development workflows**: Speed up development and testing by caching embeddings during experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's clean up our caches to avoid leaving data in Redis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "// Clean up only the caches we created in this notebook\n",
    "// Note: cache.clear() is not available, we need to clean up entries individually\n",
    "\n",
    "// Close Redis connection\n",
    "jedis.close();\n",
    "\n",
    "System.out.println(\"Connection closed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `EmbeddingsCache` provides an efficient way to store and retrieve embeddings with their associated text. Key features include:\n",
    "\n",
    "- Simple API for storing and retrieving individual embeddings (`set`/`get`)\n",
    "- Batch operations for working with multiple embeddings efficiently (`mset`/`mget`/`mexists`/`mdrop`)\n",
    "- Configurable time-to-live (TTL) for cache entries\n",
    "- Integration with RedisVL vectorizers for automatic caching\n",
    "- Significant performance improvements (up to 10x faster with caching)\n",
    "\n",
    "By using the `EmbeddingsCache`, you can reduce computational costs and improve the performance of applications that rely on embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "21.0.8+9-Ubuntu-0ubuntu124.04.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
